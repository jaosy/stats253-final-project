---
title: "STAT 253 Final Project"
author: "Ethan Hyslop, Leah Robotham, Jacqueline Ong, Emma Iverson"
date: "5/5/2022"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
    df_print: paged
    code_download: true
    code_folding: "hide"
---

```{r setup, include=FALSE, appendix = TRUE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE, tidy = TRUE)
```

# Introduction

```{r, appendix = TRUE}
# library statements 
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(tidymodels)
library(lubridate)
library(gridExtra)
library(probably)
library(vip)
library(ranger)
tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions
```

In this analysis, we utilize statistical machine learning techniques to study the spread of COVID-19 cases between August 13th, 2021 and January 27th, 2022, when we downloaded the source data. We selected this date range to best capture the current climate of the pandemic, selecting a date after vaccine boosters became available, and during when COVID-19 Delta and Omicron variants began disrupting previously established COVID trend data. We hope that with this research we may contribute to a growing understanding of what factors lead to COVID spread, and a later analysis seeks to illuminate how case trends cluster across different continents, further developing COVID-spread as a geographically heterogeneous phenomenon. 
  
Our analysis in this project centers around three research questions. Our first line of query utilizes a LASSO regression model to predict the quantity of new cases within the United States. Here, units represent individual predicted cases which may be explained by variation in the predictor variables detailed in the section on regression below. This first question seeks to understand what impacts the spread of COVID variants, addressing the core motivation of our research to expand this knowledge. Next, we used a random forest decision tree to classify country-level total case rate by continent. This was performed to understand how variation in new case rates in different regions of the world may vary in causal factors, seen here in how the model classifies cases. Ultimately, this section seeks to empower a better geographically-specific response to the pandemic. Finally, we built on this classification analysis by using K-means clustering to group observed international vaccination rates and total cases by continent. This was done in an effort to understand natural groups within the data, associated with case trends seen globally.

### Societal Impact

These analyses focus on the COVID-19 pandemic which has resulted in widespread illness and death. The data reflects the very real tragedies of people around the world, and as such should be considered with gravity. These data have been collected in an anonymous way to respect the privacy of those they represent, and can be used in the important effort to better understand the spread and nature of the disease. We do not anticipate incurring any societal harm through the use and analysis of this data. 


### Data Cleaning
```{r, appendix = TRUE}
# read in data
covid <- read_csv("owid-covid-data.csv")
```
A case in our data set for regression represents the COVID data from the United States on a single day within our date range. We chose to use only cases from the United States within this specific date range to minimize the number of NULL values in our data set. The quantitative outcome variable for regression which we are trying to predict is the number of new cases on any given day. We used 10 predictor variables: `total_cases,` `new_cases,` `date,` `total_deaths,` `reproduction_rate,`
`people_vaccinated_per_hundred,` `people_fully_vaccinated_per_hundred,` `total_boosters_per_hundred,` `tests_per_case,` `icu_patients` and `positive_rate.` The data was compiled by the Our World in Data project from the official numbers from governments and health ministries worldwide in an effort to document and understand COVID trends. 
```{r, appendix = TRUE}
# data cleaning
covid <- covid %>% 
  filter(location == 'United States') %>%
  select(total_cases, new_cases, date, total_deaths, reproduction_rate, people_vaccinated_per_hundred, people_fully_vaccinated_per_hundred, total_boosters_per_hundred, tests_per_case, icu_patients, positive_rate) %>%
  filter(date >= as.Date("2021-08-13"))%>%
  na.omit(covid)
```
Coming into this project, we suspect that there will be a strong negative correlation between vaccination metrics and the number of total cases that are reported in any location. This hypothesis is supported by a multitude of scientific studies showing the effectiveness of COVID-19 vaccines at “preventing infection, serious illness, and death” (Center for Disease Control, https://www.cdc.gov/coronavirus/2019-ncov/vaccines/effectiveness/why-measure-effectiveness/breakthrough-cases.html).  

# Regression

```{r, appendix = TRUE}
# creation of cv folds
set.seed(123)
covid_cv10 <- vfold_cv(covid, v = 10)
```

### Methods

First, we implement regression models to predict the quantity of new cases reported within the United States, addressing our first research question. We tried Ordinary Least Squares and LASSO methods, using cross-validation and evaluative metrics including MAE and residual plots to identify the model that performs the best.  While we ultimately selected the LASSO model due to its slightly higher predictive ability, we also share some limited background of the OLS model to contextualize our final decision.



### OLS
 
This section details the process completed to construct the OLS models, beginning with this feature coefficient estimate plot, and the following output table which provides an overview of metrics for the selected predictor variables.
```{r, appendix = TRUE}
# model spec
# OLS
lm_spec <-
    linear_reg() %>% 
    set_engine(engine = 'lm') %>% 
    set_mode('regression')

mod1 <- fit(lm_spec,
            new_cases ~ ., 
            data = covid)

mod2 <- fit(lm_spec,
            new_cases ~ reproduction_rate, 
            data = covid)

mod3 <- fit(lm_spec,
            new_cases ~ reproduction_rate+people_vaccinated_per_hundred, 
            data = covid)

mod4 <- fit(lm_spec,
            new_cases ~ reproduction_rate+people_vaccinated_per_hundred+people_fully_vaccinated_per_hundred, 
            data = covid)

mod5 <- fit(lm_spec,
            new_cases ~ reproduction_rate+people_vaccinated_per_hundred+people_fully_vaccinated_per_hundred+total_boosters_per_hundred, 
            data = covid)


covid_rec_ols <- recipe(new_cases ~ .,  data = covid) %>%
  step_normalize(all_numeric_predictors()) 

#mod1
cov_wf_ols <- workflow() %>%
  add_recipe(covid_rec_ols) %>%
  add_model(lm_spec)%>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors())



mod1 <- fit(cov_wf_ols,
            data = covid) 

#mod2
cov_wf_ols2 <- workflow() %>%
  add_formula(new_cases ~ reproduction_rate)%>%
  add_model(lm_spec)%>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors())

cov_wf_ols3 <- workflow() %>%
  add_formula(new_cases ~ reproduction_rate+people_vaccinated_per_hundred)%>%
  add_model(lm_spec)%>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors())

cov_wf_ols4 <- workflow() %>%
  add_formula(new_cases ~ reproduction_rate+people_vaccinated_per_hundred+people_fully_vaccinated_per_hundred)%>%
  add_model(lm_spec)%>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors())

cov_wf_ols5 <- workflow() %>%
  add_formula(new_cases ~ reproduction_rate+people_vaccinated_per_hundred+people_fully_vaccinated_per_hundred+total_boosters_per_hundred)%>%
  add_model(lm_spec)%>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_nzv(all_predictors())

```

```{r, appendix = TRUE}
mod1 %>% 
  tidy() %>% 
  slice(-1) %>% 
  mutate(lower = estimate - 1.96*std.error, upper = estimate + 1.96*std.error) %>% 
  ggplot() + 
    geom_vline(xintercept=0, linetype=4) + 
    geom_point(aes(x=estimate, y=term)) + 
    geom_segment(aes(y=term, yend=term, x=lower, xend=upper), arrow = arrow(angle=90, ends='both', length = unit(0.1, 'cm'))) + 
    labs(x = 'Coefficient estimate (95% CI)', y = 'Feature') +  
    theme_classic() 

mod1 %>%
  tidy()
```

Next, we considered a variety of models, each consisting of a different selection of predictor variables. To gain an understanding of relative error rates of these models, we calculated the mean average error (MAE) for each, presented below.
```{r, appendix = TRUE}
mod1_output <- mod1 %>% 
    predict(new_data = covid) %>%
    bind_cols(covid) %>%
    mutate(resid = new_cases - .pred)

mod2_output <- mod2 %>% 
    predict(new_data = covid) %>%
    bind_cols(covid) %>%
    mutate(resid = new_cases - .pred)

mod3_output <- mod3 %>% 
    predict(new_data = covid) %>%
    bind_cols(covid) %>%
    mutate(resid = new_cases - .pred)

mod4_output <- mod4 %>% 
    predict(new_data = covid) %>%
    bind_cols(covid) %>%
    mutate(resid = new_cases - .pred)

mod5_output <- mod5 %>% 
    predict(new_data = covid) %>%
    bind_cols(covid) %>%
    mutate(resid = new_cases - .pred)
```

```{r, appendix = TRUE}
mod1_output %>%
    mae(truth = new_cases, estimate = .pred)

mod2_output %>%
    mae(truth = new_cases, estimate = .pred)

mod3_output %>%
    mae(truth = new_cases, estimate = .pred)

mod4_output %>%
    mae(truth = new_cases, estimate = .pred)

mod5_output %>%
    mae(truth = new_cases, estimate = .pred)
```

After receiving these error metrics, we examined residual plots to determine if there were any issues of systematic over- or under-estimation, seeking a standard distribution of residuals. As seen below, all variables presented generally flat residual lines—with some fanning among higher values—suggesting that while there were no major systematic issues with the model, it had difficulty predicting these higher values accurately. 

```{r, results='hide', appendix = TRUE}
#mod1
# Hidden plots because we did not select model

mod1_tc <- ggplot(mod1_output, aes(y=resid, x=total_cases)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

mod1_td <- ggplot(mod1_output, aes(y=resid, x=total_deaths)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

mod1_rr <- ggplot(mod1_output, aes(y=resid, x=reproduction_rate)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

mod1_pvph <- ggplot(mod1_output, aes(y=resid, x=people_vaccinated_per_hundred)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

mod1_pfvph <- ggplot(mod1_output, aes(y=resid, x=people_fully_vaccinated_per_hundred)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

mod1_tbph <- ggplot(mod1_output, aes(y=resid, x=total_boosters_per_hundred)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

mod1_tpc <- ggplot(mod1_output, aes(y=resid, x=tests_per_case)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

mod1_date <- ggplot(mod1_output, aes(y=resid, x=date)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

mod1_icu <- ggplot(mod1_output, aes(y=resid, x=icu_patients)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

grid.arrange(mod1_tc, mod1_td, mod1_rr, mod1_pvph, nrow =2, ncol = 2)

grid.arrange(mod1_pfvph, mod1_tbph, mod1_tpc, mod1_date, nrow=2, ncol=2)

mod1_icu
```

However, when we examined for linearity, we discovered several instances where residual patterns suggested non-linear relationships. 

```{r, appendix = TRUE}
#mod2
ggplot(mod2_output, aes(y=resid, x=reproduction_rate)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

```

```{r, results='hide', appendix = TRUE}
# Hidden plots because we did not select model

#mod3
ggplot(mod3_output, aes(y=resid, x=reproduction_rate)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(mod3_output, aes(y=resid, x=people_vaccinated_per_hundred)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

```

```{r, results='hide', appendix = TRUE}
# Hidden plots because we did not select model

#mod4

ggplot(mod4_output, aes(y=resid, x=reproduction_rate)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(mod4_output, aes(y=resid, x=people_vaccinated_per_hundred)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(mod4_output, aes(y=resid, x=people_fully_vaccinated_per_hundred)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()


```

```{r, results='hide', appendix = TRUE}
# Hidden plots because we did not select model
#mod5
ggplot(mod5_output, aes(y=resid, x=reproduction_rate)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(mod5_output, aes(y=resid, x=people_vaccinated_per_hundred)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(mod5_output, aes(y=resid, x=people_fully_vaccinated_per_hundred)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

ggplot(mod5_output, aes(y=resid, x=total_boosters_per_hundred)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

```

From the residual plots, it seems that `people_fully_vaccinated_per_hundred	`, `people_vaccinated_per_hundred` and	
`total_boosters_per_hundred` are variables that would be better modeled with nonlinear relationships.

Finally, we calculated further metrics to consider these five models.

```{r, appendix = TRUE}
mod1_cv <- fit_resamples(cov_wf_ols,
  resamples = covid_cv10, 
  metrics = metric_set(rmse, rsq, mae))
  
  
mod2_cv <- fit_resamples(cov_wf_ols2,
  resamples = covid_cv10, 
  metrics = metric_set(rmse, rsq, mae))
  
  
mod3_cv <- fit_resamples(cov_wf_ols3,
  resamples = covid_cv10, 
  metrics = metric_set(rmse, rsq, mae))
  
  
mod4_cv <- fit_resamples(cov_wf_ols4,
  resamples = covid_cv10, 
  metrics = metric_set(rmse, rsq, mae))
  
  
mod5_cv <- fit_resamples(cov_wf_ols5,
  resamples = covid_cv10, 
  metrics = metric_set(rmse, rsq, mae))

```

```{r, appendix = TRUE}

mod1_cv %>% unnest(.metrics) %>%
  filter(.metric == 'rmse') %>%
  summarize(RMSE_CV = mean(.estimate))

mod2_cv %>% unnest(.metrics) %>%
  filter(.metric == 'rmse') %>%
  summarize(RMSE_CV = mean(.estimate))

mod3_cv %>% unnest(.metrics) %>%
  filter(.metric == 'rmse') %>%
  summarize(RMSE_CV = mean(.estimate))

mod4_cv %>% unnest(.metrics) %>%
  filter(.metric == 'rmse') %>%
  summarize(RMSE_CV = mean(.estimate))

mod5_cv %>% unnest(.metrics) %>%
    filter(.metric == 'rmse') %>%
    summarize(RMSE_CV = mean(.estimate))

mod1_cv %>% collect_metrics()

mod2_cv %>% collect_metrics()

mod3_cv %>% collect_metrics()

mod4_cv %>% collect_metrics()

mod5_cv %>% collect_metrics()


```

### LASSO

After completing our OLS model, we focused on constructing a LASSO model to explore a different way of examining these associations. First, we attempted to estimate an ideal tuning parameter value, lambda, as seen in the plot below. 

```{r, appendix = TRUE}
# LASSO
lm_lasso_spec <- 
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>% 
  set_engine(engine = 'glmnet') %>% 
  set_mode('regression') 

```

```{r, appendix = TRUE}
# recipes & workflows
# specify Recipe (if you have preprocessing steps)

covid <- covid %>% select(-date)

covid_recipe <- recipe(new_cases ~ ., data = covid) %>%
  step_nzv(all_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_naomit(all_predictors()) #skip = TRUE


covid_wf <- workflow() %>%
  add_recipe(covid_recipe) %>%
  add_model(lm_spec)

covid_wf_lasso <- workflow() %>%
  add_recipe(covid_recipe) %>%
  add_model(lm_lasso_spec)
```

```{r, appendix = TRUE}
# fit & tune models

mod_lasso_fit <- fit(covid_wf_lasso, data = covid)

plot(mod_lasso_fit %>% extract_fit_parsnip() %>% pluck('fit'), # way to get the original glmnet output
     xvar = "lambda") # glmnet fits the model with a variety of lambda penalty values

# tune model (try a variety of lambda penalty values)
penalty_grid <- grid_regular(penalty(range = c(-5,3)), levels = 30)
```


```{r, appendix = TRUE}

tune_res <- tune_grid( # new function for tuning parameters
  covid_wf_lasso, # workflow
  resamples = covid_cv10, # cv folds
  metrics = metric_set(rmse, mae),
  grid = penalty_grid # penalty grid defined above
)

#  calculate/collect CV metrics
collect_metrics(tune_res) %>%
  filter(.metric == 'mae') %>% # or choose mae
  select(penalty, mae = mean, std_err) 

# best penalty
best_penalty <- select_best(tune_res, metric = 'mae') # choose penalty value based on lowest mae or rmse
best_se_penalty <- select_by_one_std_err(tune_res, metric = 'mae', desc(penalty)) # choose penalty value based on the largest penalty within 1 se of the lowest CV MAE
```

Based on these outcomes, we selected a final model with the largest penalty within one standard error of the lowest cross-validated MAE.

```{r, appendix = TRUE}
#Fit Final Model
final_wf <- finalize_workflow(covid_wf_lasso, best_penalty) # incorporates penalty value to workflow
final_wf_se <- finalize_workflow(covid_wf_lasso, best_se_penalty) # incorporates penalty value to workflow

final_fit <- fit(final_wf, data = covid)
final_fit_se <- fit(final_wf_se, data = covid)

tidy(final_fit)
tidy(final_fit_se)
```

 

```{r, appendix = TRUE}
# visual residuals

tune_res %>% collect_metrics() %>% filter(penalty == (best_se_penalty %>% pull(penalty)))
```

### Residual Plot of LASSO Model Prediction

Next, we examined residuals within our final model. Here, we determined that the LASSO model was the best model for our data so far, because it had the lowest MAE compared to the other models we tried.

```{r, appendix = TRUE}
lasso_mod_out <- final_fit_se %>%
    predict(new_data = covid) %>%
    bind_cols(covid) %>%
    mutate(resid = new_cases - .pred)

lasso_mod_out %>% 
  ggplot(aes(x = .pred, y = resid)) + 
  geom_point() +
  geom_smooth(se = FALSE) + 
  geom_hline(yintercept = 0) + 
  theme_classic()
```

 

```{r, results='hide', appendix = TRUE}
# Hiding residual plots for model we did not choose

pl1 <- ggplot(lasso_mod_out, aes(y=resid, x=total_cases)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

pl2 <- ggplot(lasso_mod_out, aes(y=resid, x=total_deaths)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

pl3 <- ggplot(lasso_mod_out, aes(y=resid, x=reproduction_rate)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

pl4<- ggplot(lasso_mod_out, aes(y=resid, x=people_vaccinated_per_hundred)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

pl5 <- ggplot(lasso_mod_out, aes(y=resid, x=people_fully_vaccinated_per_hundred)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

pl6 <- ggplot(lasso_mod_out, aes(y=resid, x=total_boosters_per_hundred)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

pl7 <- ggplot(lasso_mod_out, aes(y=resid, x=tests_per_case)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

pl8 <- ggplot(lasso_mod_out, aes(y=resid, x=icu_patients)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

pl9 <- ggplot(lasso_mod_out, aes(y=resid, x=positive_rate)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") + 
    theme_classic()

pl1
pl2
pl3
pl4
pl5
pl6
pl7
pl8
pl9
```

From the residual plots of the LASSO model above, we found that  we could try to model all of these predictors with nonlinear relationships. This is because the plots show heteroskedasticity. 


We can look at the LASSO regression results to determine variable importance. LASSO is a shrinking algorithm which will eliminate weak predictors from the model by setting their coefficients to 0. From the estimate values of the coefficients, we can see that `total_deaths`, `people_vaccinated_per_hundred` and `total_boosters_per_hundred` have been shrunk to 0 and are not considered important variables in the model. 

However, the magnitude of the other coefficients produced by the LASSO algorithm are quite large, so this indicates that they are all important variables.



<br>

### GAMS
After acknowledging the prevalence of nonlinearity and heteroskedasticity throughout our regression model selection process, we chose to fit a model that allows for non-linearity. In this section, we implement GAMs and splines to account for this. 
```{r, appendix = TRUE}
# GAMS

gam_spec <- 
  gen_additive_mod() %>%
  set_engine(engine = 'mgcv') %>%
  set_mode('regression') 

gam_mod <- fit(gam_spec,
    new_cases ~ s(total_cases) + s(total_deaths) + s(total_deaths) + s(reproduction_rate) + s(people_vaccinated_per_hundred) + s(people_fully_vaccinated_per_hundred) + s(tests_per_case) + s(icu_patients) + s(positive_rate) + s(total_boosters_per_hundred),
    data = covid
)

# Diagnostics: Check to see if the number of knots is large enough (if p-value is low, increase number of knots)
par(mfrow=c(2,2))
gam_mod %>% pluck('fit') %>% mgcv::gam.check() 

```
```{r}
gam_mod %>% pluck('fit') %>% summary() 
#gam_mod %>% pluck('fit') %>% plot()
gam_mod %>% pluck('fit') %>% plot( all.terms = TRUE, pages = 1)
```

```{r, appendix = TRUE}
#fitting a more simple GAM model by only allowing for a non-linear function if the edf is greater than 3.
gam_mod2 <- fit(gam_spec,
    new_cases ~ s(total_cases) + s(total_deaths) + s(total_deaths) + s(reproduction_rate) + s(people_vaccinated_per_hundred) + people_fully_vaccinated_per_hundred + tests_per_case + icu_patients + s(positive_rate) + s(total_boosters_per_hundred),
    data = covid
)

gam_mod2 %>% pluck('fit') %>% summary() 
gam_mod2 %>% pluck('fit') %>% plot()
``` 


```{r, appendix = TRUE}

lm_spec <-
  linear_reg() %>%
  set_engine(engine = 'lm') %>%
  set_mode('regression')

data_cv8 <- covid %>% 
    vfold_cv(v = 8)

covid_rec_spline <- recipe(new_cases ~ total_cases + total_deaths + reproduction_rate + people_vaccinated_per_hundred + people_fully_vaccinated_per_hundred+ total_boosters_per_hundred+ tests_per_case + icu_patients+ positive_rate, data = covid)
  
spline_rec <- covid_rec_spline %>% 
  step_ns(total_cases, deg_free = 9) %>%
  step_ns(total_deaths, deg_free = 5) %>% 
  step_ns(reproduction_rate, deg_free = 9) %>% 
  step_ns(people_vaccinated_per_hundred, deg_free = 2) %>% 
  step_ns(total_boosters_per_hundred, deg_free = 8) %>%
  step_ns(positive_rate, deg_free = 1)

# Check the pre-processed data
spline_rec %>% prep(covid) %>% juice()

covid_spline_wf <- workflow() %>%
    add_model(lm_spec) %>%
    add_recipe(covid_rec_spline)
  
spline_wf <- workflow() %>% 
  add_recipe(spline_rec) %>%
  add_model(lm_spec) 

fit_resamples(
    covid_spline_wf,
    resamples = data_cv8, # cv folds
    metrics = metric_set(mae,rmse,rsq)                     
) %>% collect_metrics()

```

```{r, appendix = TRUE}
fit_resamples(
    spline_wf,
    resamples = data_cv8, # cv folds
    metrics = metric_set(mae,rmse,rsq)                     
) %>% collect_metrics()
```

After recieving the GAM model results above, we examined the residuals as before, looking for systematic prediction errors and nonlinearity.

```{r, results='hide', appendix = TRUE}
# We print to compare to LASSO so hiding the print out of the individual plots

ns_mod <- spline_wf %>%
  fit(data = covid) 

ns_mod %>%
  tidy()

spline_mod_output <- covid %>%
  bind_cols(predict(ns_mod, new_data = covid)) %>%
    mutate(resid = new_cases - .pred)

p4 <- ggplot(spline_mod_output, aes(x = total_cases, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
p5 <- ggplot(spline_mod_output, aes(x = total_deaths, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
p6 <- ggplot(spline_mod_output, aes(x = reproduction_rate, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
p7 <- ggplot(spline_mod_output, aes(x = people_vaccinated_per_hundred, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
p8 <- ggplot(spline_mod_output, aes(x = total_boosters_per_hundred, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
p9 <- ggplot(spline_mod_output, aes(x = positive_rate, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
p4
p5
p6
p7
p8
p9
```

```{r, fig.width = 4,fig.height = 1.5, appendix = TRUE}
grid.arrange(pl1, p4, nrow = 1, ncol = 2)
grid.arrange(pl2, p5, nrow = 1, ncol = 2)
grid.arrange(pl3, p6, nrow = 1, ncol = 2)
grid.arrange(pl4, p7, nrow = 1, ncol = 2)
grid.arrange(pl6, p8, nrow = 1, ncol = 2)
grid.arrange(pl9, p9, nrow = 1, ncol = 2)
```

### Regression - Results

Comparing our LASSO model and nonlinear splines model, we see the inclusion of previously zeroed out variables including total deaths, people vaccinated per hundred, and people boosted per hundred. However, our most predictive variables are still the greatest in the model.  

The GAM output plots reveal a handful of non-linear variables, especially total vaccinations per hundred, and total boosters per hundred. This demonstrates the value of fitting models allowing for non-linearity, as forcing linearity upon these would have resulted in a poor estimate. 

Across all metrics, our GAM models perform somewhat better than those which assume non-linearity. We can see this in the lower MAE, lower RMSE, and slightly higher r-squared.

In this final model, people fully vaccinated per hundred is notably included among the most predictive variables in general, and is significantly different as a nonlinear function.

### Regression - Conclusions

Our analysis goal is to predict the quantity of new COVID cases. Given the universal salience of this subject, it is important to have both highly interpretable results as well as a quality predictive model. To attempt to explain the rate of new COVID cases, we have fit a series of least-squares linear regression models, as well as a series of models subjected to LASSO linear regression. Of our fit models, we find that the best-fit LASSO model has the lowest mean absolute error (MAE), therefore producing the most accurate prediction of our fit models. However, even this model results in a sizable MAE. To correct this, we must consider the non linearity of some input variables, and undertake steps to model them using polynomial regression. 

We attempted to correct for nonlinearity through the implementation of splines, however these models had an even larger MAE. Therefore, we will continue using the last LASSO model. 


It is important to remember that the reporting of these data has been done individually by each country, and therefore is incomplete and prone to some inconsistencies. Additionally, it is important to remember the people behind these data, both for context and in recognition of the lives lost. 


# Classification


```{r, appendix = TRUE}
# read in data
covid <- read_csv("owid-covid-data.csv")

# data cleaning
covid <- covid %>%
  select(total_cases, new_cases, date, total_deaths, reproduction_rate, people_vaccinated_per_hundred, people_fully_vaccinated_per_hundred, total_boosters_per_hundred, tests_per_case, icu_patients, positive_rate, life_expectancy, population_density, median_age, gdp_per_capita, continent) %>%
  filter(date >= as.Date("2021-08-13"))%>%
  na.omit(covid)
```

As before, we're considering COVID cases occurring after the first recorded booster vaccines (13th August, 2021) to best capture today's landscape. This is when boosters started to become widespread, at least in the United States, and also saw the dawn of serious COVID variants. In addition to our existing variables, we also added the following to our model to predict the continent:
    - `life_expectancy`
    - `population_density`
    - `median_age`
    - `gdp_per_capita`


## Classification - Methods

As with our regression analysis we tried two models, logistic regression and random forests classification. These models were employed to predict the origin continent of each case. Ultimately, we chose random forests in large part because it can produce non-binary results. 

### LASSO Logistic Regression 

We fit a LASSO logistic regression model for the `continent` outcome, and allowed all possible predictors to be considered.

We initially tried a sequence of 100 $\lambda$s from 1 to 10, which we then adjusted based on the plot of test AUC versus $\lambda$.

Since we can only perform binominal logistic regression, we had to create a new variable, `NorthAmerica_or_Not`, categorizing the data based on whether its `continent` was "North America" or not. This new variable will be the outcome we try to predict using this method, setting the reference level as not North America. This sort of classification may be important in a real life context for administrative health agencies to spot trends and disparities between North America, a historically powerful continent with the resources to implement and enforce COVID vaccinations, and other continents that struggle more to do so.

To fit the model, we had to remove `date` and `continent` as predictors, since their formats did not work with the engine.

```{r, appendix = TRUE}
# Create new binary variable
covid <-covid %>% 
  mutate(NorthAmerica_or_Not = ifelse(continent == 'North America', 'North_America', 'Not_North_America'))

# Set reference level (to the outcome you are NOT interested in)
covid <- covid %>%
  mutate(NorthAmerica_or_Not = relevel(factor(NorthAmerica_or_Not), ref='Not_North_America')) #set reference level

# creation of cv folds
set.seed(123)
covid_cv10 <- vfold_cv(covid, v = 10)

# Logistic LASSO Regression Model Spec
logistic_lasso_spec_tune <- logistic_reg() %>%
    set_engine('glmnet') %>%
    set_args(mixture = 1, penalty = tune()) %>%
    set_mode('classification')

# Recipe
logistic_rec <- recipe(NorthAmerica_or_Not ~ total_cases + new_cases + total_deaths + reproduction_rate + people_vaccinated_per_hundred + people_fully_vaccinated_per_hundred + total_boosters_per_hundred + tests_per_case + icu_patients + positive_rate + life_expectancy + population_density + median_age + gdp_per_capita, data = covid) %>%
    step_normalize(all_numeric_predictors()) %>% 
    step_dummy(all_nominal_predictors())

# Workflow (Recipe + Model)
log_lasso_wf <- workflow() %>% 
    add_recipe(logistic_rec) %>%
    add_model(logistic_lasso_spec_tune) 

# Tune Model (trying a variety of values of Lambda penalty)
penalty_grid <- grid_regular(
  penalty(range = c(-5, 1)), #log10 transformed  (kept moving min down from 0)
  levels = 100)

tune_output <- tune_grid( 
  log_lasso_wf, # workflow
  resamples = covid_cv10, # cv folds
  metrics = metric_set(roc_auc,accuracy),
  control = control_resamples(save_pred = TRUE, event_level = 'second'),
  grid = penalty_grid # penalty grid defined above
)

# Visualize Model Evaluation Metrics from Tuning
autoplot(tune_output) + theme_classic()
```


#### Inspecting the model 

We will select the final model possessing a CV AUC within one standard error of the overall best metric. 

```{r, appendix = TRUE}
# Select Penalty
best_se_penalty <- select_by_one_std_err(tune_output, metric = 'roc_auc', desc(penalty)) # choose penalty value based on the largest penalty within 1 se of the highest CV roc_auc
best_se_penalty

# Fit Final Model
final_fit_se <- finalize_workflow(log_lasso_wf, best_se_penalty) %>% # incorporates penalty value to workflow 
    fit(data = covid)

final_fit_se %>% tidy() %>%
  filter(estimate == 0)
```
With the set of penalty values that we used, the following variables had their coefficients set to 0:
- `total_cases`
- `people_fully_vaccinated_per_hundred`

The total number of cases and number of people fully vaccinated per hundred were proven less important in predicting whether or not an observation originated from North America or not, after accounting for the other features in the model.

Here we will look at how long a variable stayed in the model as a measure of variable importance:

```{r, appendix = TRUE}
glmnet_output <- final_fit_se %>% extract_fit_engine()
    
# Create a boolean matrix (predictors x lambdas) of variable exclusion
bool_predictor_exclude <- glmnet_output$beta==0

# Loop over each variable
var_imp <- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) {
    # Extract coefficient path (sorted from highest to lowest lambda)
    this_coeff_path <- bool_predictor_exclude[row,]
    # Compute and return the # of lambdas until this variable is out forever
    ncol(bool_predictor_exclude) - which.min(this_coeff_path) + 1
})

# Create a dataset of this information and sort
var_imp_data <- tibble(
    var_name = rownames(bool_predictor_exclude),
    var_imp = var_imp
)
var_imp_data %>% arrange(desc(var_imp))
```

Based on the output above, the variables deemed the most important in predicting the continent based on how long they persisted in the LASSO algorithm as the penalty increased. 

unsurprisingly, `total_cases` and `people_fully_vaccinated_per_hundred` *are* important, though counter to initial results.

#### Interpreting evaluation metrics

We inspect the overall CV results for the "best" $\lambda$, and compute the no-information (NIR). 

```{r, appendix = TRUE}
# CV results for "best lambda"
tune_output %>%
    collect_metrics() %>%
    filter(penalty == best_se_penalty %>% pull(penalty))
           
# Count up the continent outcomes
covid %>% count(NorthAmerica_or_Not) # Name of the outcome variable goes inside count()

# Compute the NIR
2596/(2596+325)
```

The relative frequency of the majority class is 0.89, meaning 89% of our observations originate from outside of North America. 

The AUC for our model is 1.0, indicating that our model is a perfect classifier of whether or not a case originated from North America.

#### Choosing a threshold and final model

After using LASSO and balancing bias and variance, we use the final model to make predictions: 

```{r, appendix = TRUE}
# Soft Predictions on Training Data
final_output <- final_fit_se %>% predict(new_data = covid, type='prob') %>% bind_cols(covid)

# FIX THIS
final_output %>%
  ggplot(aes(x = NorthAmerica_or_Not, y = .pred_North_America)) +
  geom_boxplot()

# ROC curve of sensitivity and specificity
final_output %>%
    roc_curve(NorthAmerica_or_Not,.pred_North_America,event_level = 'second') %>%
    autoplot()
```

This ROC curve is 1.0, indicating that our model is a perfect classifier with sensitivity of 1. 

Sensitivity is the measure of how North-American cases continents are correctly predicted as North-American by our model. 

Specificity is measure of how often non-North-American cases are correctly predicted to be not North-American by our model. 

We can also calculate the J index:

```{r, appendix = TRUE}
# thresholds in terms of reference level
threshold_output <- final_output %>%
    threshold_perf(truth = NorthAmerica_or_Not, estimate = .pred_North_America, thresholds = seq(0,1,by=.01)) 

# J-index v. threshold for not_spam
threshold_output %>%
    filter(.metric == 'j_index') %>%
    ggplot(aes(x = .threshold, y = .estimate)) +
    geom_line() +
    labs(y = 'J-index', x = 'threshold') +
    theme_classic()
```

```{r, appendix = TRUE}
threshold_output %>%
    filter(.metric == 'j_index') %>%
    arrange(desc(.estimate))
```

```{r, appendix = TRUE}
# Distance v. threshold for not_North_America

threshold_output %>%
    filter(.metric == 'distance') %>%
    ggplot(aes(x = .threshold, y = .estimate)) +
    geom_line() +
    labs(y = 'Distance', x = 'threshold') +
    theme_classic()
```

```{r, appendix = TRUE}
threshold_output %>%
    filter(.metric == 'distance') %>%
    arrange(.estimate)
```

```{r, appendix = TRUE}
log_metrics <- metric_set(accuracy,sens,yardstick::spec)

final_output %>%
    mutate(.pred_class = make_two_class_pred(.pred_Not_North_America, levels(NorthAmerica_or_Not), threshold = .98)) %>%
    log_metrics(truth = NorthAmerica_or_Not, estimate = .pred_class, event_level = 'second')
```

95% of COVID case classifications selecting North America or not are expected to be correct under this model.

Our NIR is 88%, so our model performs better and is actually almost 100% accurate. However, most of our cases do not actually originate from North America.

Our model classifies COVID cases as originating from North America or not correctly 95% of the time. The ROC AUC also has an output of 1.0, meaning our model performs perfect continent classification. 

These results are outstanding at first glance because it seems like our model is able to classify COVID cases by continent almost perfectly. However, in context, it turns out that 89% of the observations in the data actually originate from outside North America. If we simply guessed picking the majority class each time, we would still make a right classification 89% of the time (following the No Information Rate). Our model performs marginally better than that, but is not particularly impressive in this context. 

### Random Forests
Next, we implimented a random forest model.

```{r, appendix = TRUE}
conflicted::conflict_prefer("vi", "vip")

# read in data
covid <- read_csv("owid-covid-data.csv")
```


```{r, appendix = TRUE}
# data cleaning
covid <- covid %>%
  filter(date >= as.Date("2021-08-13"))%>%
  select(-icu_patients, -icu_patients_per_million, -hosp_patients, -hosp_patients_per_million, -weekly_icu_admissions, -weekly_icu_admissions_per_million, -weekly_hosp_admissions, -weekly_hosp_admissions_per_million, -total_tests, -total_tests_per_thousand, -new_tests_smoothed, -new_tests_smoothed_per_thousand, -handwashing_facilities, -excess_mortality_cumulative_absolute, -excess_mortality_cumulative, -excess_mortality, -excess_mortality_cumulative_per_million, -male_smokers, -female_smokers, -hospital_beds_per_thousand, -extreme_poverty, -location)

```

```{r, appendix = TRUE}
covid <- covid %>%
  na.omit(covid) %>%
  filter(continent %in% c("North America", "Asia", "South America", "Africa", "Europe", "Oceania")) %>%
  mutate(continent = factor(continent))
```
We use the same set of predictors here as with our logistic regression model. 


```{r, appendix = TRUE}
# Bagging and Random Forest

# Model Specification
rf_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args(mtry = NULL, # size of random subset of variables; default is floor(sqrt(ncol(x)))
           trees = 1000, # Number of trees
           min_n = 2,
           probability = FALSE, # FALSE: hard predictions
           importance = 'impurity') %>% 
  set_mode('classification') # change this for regression tree

# Recipe

rf_rec <- recipe(continent ~ ., data = covid)

# Workflows
data_wf_mtry2 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 2)) %>%
  add_recipe(rf_rec)

# Create workflows for mtry = 7, 22, and 44
data_wf_mtry7 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 7)) %>%
  add_recipe(rf_rec)

data_wf_mtry22 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 22)) %>%
  add_recipe(rf_rec)

data_wf_mtry44 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 44)) %>%
  add_recipe(rf_rec)
```

```{r, appendix = TRUE}
# Fit Models

set.seed(123) # make sure to run this before each fit so that you have the same 1000 trees
data_fit_mtry2 <- fit(data_wf_mtry2, data = covid)

set.seed(123)
data_fit_mtry7 <- fit(data_wf_mtry7, data = covid)

set.seed(123) 
data_fit_mtry22 <- fit(data_wf_mtry22, data = covid)

set.seed(123)
data_fit_mtry44 <- fit(data_wf_mtry44, data = covid)
```

```{r, appendix = TRUE}
# Custom Function to get OOB predictions, true observed outcomes and add a model label
rf_OOB_output <- function(fit_model, model_label, truth){
    tibble(
          .pred_continent = fit_model %>% extract_fit_engine() %>% pluck('predictions'), #OOB predictions
          continent = truth,
          model = model_label
      )
}

#check out the function output
rf_OOB_output(data_fit_mtry2,'mtry2', covid %>% pull(continent))
```

```{r, appendix = TRUE}
# Evaluate OOB Metrics

data_rf_OOB_output <- bind_rows(
    rf_OOB_output(data_fit_mtry2,'mtry2', covid %>% pull(continent)),
    rf_OOB_output(data_fit_mtry7,'mtry7', covid %>% pull(continent)),
    rf_OOB_output(data_fit_mtry22,'mtry22', covid %>% pull(continent)),
    rf_OOB_output(data_fit_mtry44,'mtry44', covid %>% pull(continent))
)


data_rf_OOB_output %>% 
    group_by(model) %>%
    accuracy(truth = continent, estimate = .pred_continent)
```
```{r}
# Preliminary Interpretation

data_rf_OOB_output %>% 
    group_by(model) %>%
    accuracy(truth = continent, estimate = .pred_continent) %>%
  mutate(mtry = as.numeric(stringr::str_replace(model,'mtry',''))) %>%
  ggplot(aes(x = mtry, y = .estimate )) + 
  geom_point() +
  geom_line() +
  theme_classic()
```

```{r, appendix = TRUE}
# Evaluating the forest

data_fit_mtry7
```

```{r, appendix = TRUE}
rf_OOB_output(data_fit_mtry7,'mtry7', covid %>% pull(continent)) %>%
    conf_mat(truth = continent, estimate= .pred_continent)
```

Here, we can also see the relative importance of included variables, as determined by the forest. 

```{r, appendix = TRUE}
# Variable importance measures

data_fit_mtry7 %>% 
    extract_fit_engine() %>% 
    vip(num_features = 30) + theme_classic() #based on impurity
```


```{r, appendix = TRUE}
data_wf_mtry7 %>% 
  update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
  fit(data = covid) %>% 
    extract_fit_engine() %>% 
    vip(num_features = 30) + theme_classic()
```

Further, we see how the value of these variables differ across continents, our outcome classes. 

```{r, appendix = TRUE}

## pick important variables to visualize

ggplot(covid, aes(x = continent, y = aged_70_older)) +
    geom_violin() + theme_classic()

ggplot(covid, aes(x = continent, y = total_vaccinations_per_hundred)) +
    geom_violin() + theme_classic()

ggplot(covid, aes(x = continent, y = total_deaths_per_million)) +
    geom_violin() + theme_classic()

ggplot(covid, aes(x = continent, y = total_cases)) +
    geom_violin() + theme_classic()

ggplot(covid, aes(x = continent, y = new_deaths_smoothed)) +
    geom_violin() + theme_classic()

```



## Classification - Results



For the logistic regression, we inspected the overall cross-validation results to determine the best lambda. We also determined the no-information-rate (NIR), which was 89%, meaning in this case that 89% of our cases represented "false (non-North America)" values. We also used the ROC curve to plot sensitivity (rate of true positive, predicting North American cases as North American) and specificity (true negative rate, predicting non-North-American cases as non-North American) of the overall model. It returned an ROC of 1 with a standard deviation of 0, meaning it has essentially perfect prediction. Finally, we evaluated the model accuracy measure from our logistic quantitative output, which equaled .95, with a one standard deviation estimate of +-0.00137, suggesting strong 95% predictive accuracy. 

For the decision tree, we used out-of-bag error (OOB) and the test confusion matrix. Our OOB returned as .02%, meaning the our model makes the wrong hard classification .02% of the time, which in our test confusion matrix is shown to be one wrong prediction. This suggests that the tree model has a 99.98% accuracy, making it slightly more accurate than the logistic model. However, this difference is within 5% accuracy, suggesting that either model could offer high predictive power in this research context. 


## Classification - Final Model Selection


These models also differ by metrics and variable importance. In the logistic regression, total cases, people vaccinated per hundred, and ICU patients proved to be among the most important variables. This is in contrast to the random forest model, which finds population age to generally be the most important predictor of new COVID cases.

We have selected the random forest model. This operates as a bagged decision tree, and can predict which continent each case is from. We selected this model for two reasons. First, and most importantly, this model can predict each content specifically, whereas the logistic model is restricted to a binary outcome of North America/Not North America. This a critical difference, as it allows for much more specific predictions within this data context. Secondly, this model has a slightly higher accuracy rating by 4.98%.




Overall the random forest model produces a more accurate prediction with fewer errors. The variables used are also different, focusing more on population demographic data than COVID data specifically. Finally, within the study context, this model also makes more sense due to the specificity of outcome it can offer via multiple-continent outcomes.  

## Classification - Conclusions


The model has an extremely low level of error, predicting results with 99.98% accuracy. This is far above any degree of predictive ability that could be expected, and is certainly an acceptable amount of error, especially considering the relatively low-stakes context of this study. 


Here, we see that only one case has been incorrectly predicted in the training data, demonstrating the extreme predictive ability of this random forest model. 
```{r, appendix = TRUE}
rf_OOB_output(data_fit_mtry7,'mtry7', covid %>% pull(continent)) %>%
    conf_mat(truth = continent, estimate= .pred_continent)
```



Overall, this model predicts with almost flawless accuracy the continent that each case of COVID data was collected in. In the context of this data, as well as the type of work which could draw on these findings, this model has successfully achieved its desired outcome. For medical professionals hoping to understand explanatory factors of COVID spread, or governments seeking to determine regional variation in explanatory factors, this model can provide direction with confidence.




# K-means Clustering

We chose K-means as our clustering algorithm over other methods including 1) Principal Component Analysis, because we had already eliminated so many variables during the data cleaning steps that further dimensional reduction would be unnecessary, and 2) hierarchical clustering, because when we attempted hierarchical clustering and constructed a dendrogram, it proved nearly impossible to read with no clear trends in classification. Also, it was not as accurate as k-means clustering. 


```{r, appendix = TRUE}

# Random subsample of 50 cases
set.seed(123)

covid <- covid %>%
  slice_sample(n = 100)

# select the variables to be used in clustering

covid_sub <- covid %>% 
  select(total_cases, people_fully_vaccinated_per_hundred)
```

### Picking K

The **Elbow Method** is based on the visualization below. We pick the $k$ that corresponds to the bend in the curve (the "elbow") such that you are minimizing total within-cluster sum of squares while keeping the number of clusters small for simplicity and interpretability.

This enables us to pick $k$ using a data-driven approach. We would be comparing the **total squared distance of each case from its assigned centroid** for different values of $k$. (This measure is available within the `$tot.withinss` component of objects resulting from `kmeans()`.)

Based on the visualization below, we pick $k = 6$. After this, there is not a meaningful decrease in heterogeneity (dissimilarity).

This is logical, as there are 6 continents where cases are observed in the data set.
```{r, appendix = TRUE}
# Data-specific function to cluster and calculate total within-cluster SS
covid_cluster_ss <- function(k){
    # Perform clustering
    kclust <- kmeans(scale(covid_sub), centers = k)

    # Return the total within-cluster sum of squares
    return(kclust$tot.withinss)
}

tibble(
    k = 1:15,
    tot_wc_ss = purrr::map_dbl(1:15, covid_cluster_ss)
) %>% 
    ggplot(aes(x = k, y = tot_wc_ss)) +
    geom_point() + 
    labs(x = "Number of clusters",y = 'Total within-cluster sum of squares') + 
    theme_classic()
```

### Cluster interpretation

```{r, appendix = TRUE}
set.seed(123)
kclust_k6_2vars <- kmeans(scale(covid_sub), centers = 6)

covid <- covid %>%
    mutate(kclust_6_2vars = factor(kclust_k6_2vars$cluster))


covid %>%
  count(continent,kclust_6_2vars)

covid %>%
    group_by(kclust_6_2vars) %>%
    summarize(across(c(total_cases, people_fully_vaccinated_per_hundred), mean))
```

Group 3 has the highest mean total cases and the second lowest mean total vaccination rate. Group 5 could be named low case rate and low vaccination rate as it has the lowest number of total cases and the lowest rate of people fully vaccinated. Group 6 has the highest number of people fully vaccinated per hundred but also has the third lowest total cases which is surprising to us but this is indicative that other factors can affect clustering. 

### Clustering Visualization

```{r, appendix = TRUE}
# Run k-means for k = centers = 6
kclust_k6 <- kmeans(scale(covid_sub), centers = 6)

# Add a variable (kclust_k3) to the original dataset 
# containing the cluster assignments
covid_sub <- covid_sub %>%
    mutate(kclust_6 = factor(kclust_k6$cluster))

# Visualize the cluster assignments on the original scatterplot
ggplot(covid_sub, aes(x = total_cases, y = people_fully_vaccinated_per_hundred, color = kclust_6)) +
    geom_point() + 
    theme_classic()
```

### Clustering Evaluation

The two features we chose in the clustering were `total_cases` and `people_fully_vaccinated_per_hundred`. We chose these variables because the variable importance calculation from the LASSO logistic regression done previously picked these two variables as the most important. Additionally, the clustering can only be done for quantitative variables since we are able to measure the distance between them.

It looks like there are 6 clusters with varying vaccination rate per hundred people and total number of cases.

We decided to use k-means clustering to see if we could group the cases by geographical continent based on the similarity of the predictor values so that the distance between cases is minimized. Using an elbow plot, we noticed that the optimal number of clusters lies between k = 6 to k = 8 (the dip in the curve), after which there would not have been any more meaningful decreases in heterogeneity. We chose k = 6 for our final clustering implementation, which is fitting since the cases from the COVID dataset originate from 6 continents. 

From clustering, we gain the knowledge that using `total_cases` and `total_vaccinations_per_hundred` helps us cluster by continent, which informs us that these predictor values differ significantly enough between each continent to let us identify the continents. Interestingly, our analysis indicates that the number of people fully vaccinated per hundred doesn’t necessarily mean a decrease in the number of total cases - for example, compare cluster 2 and 6, where 6 is doing much better in terms of vaccination rate, but records a similar number of total cases. This could be due to external factors like globalization or populations who are vaccinated not continuing to wear masks. 

# Conclusion

In this research project, we have pursued three lines of query. First, we sought to explain daily new COVID cases post-vaccine booster, representing largely COVID variant cases given our examined time span. Second, we wanted to investigate the geographic variation of these trends by predicting the origin continent of recorded total case counts. Finally, we examined natural continent-level clustering of case patterns. In our first analysis, we underwent several regression techniques to construct a model which accounted for nonlinearity. We found total vaccination rate to be among the most important variables in predicting cases. This builds upon extensive scientific support for the efficacy of vaccines as a central tool in curbing the COVID-19 pandemic. In the second and third analyses, our models were able to predict and cluster, respectively, total case values into continents with almost perfect accuracy. While some shortcomings in the data—noted earlier—contribute to this high predictive ability, it suggests how COVID cases are still a phenomenon experienced unequally across continents. Together, these finding mandate that we continue to support the accessibility of vaccines, as well as remain aware of the international discrepancies in the pandemic. 

## Appendix

```{r, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
