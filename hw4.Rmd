```{r hw2_setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, eval=FALSE)
```

# Homework 4

## Required Analysis

For this homework,

1. **Specify the research question for a classification task.**

Can we predict the continent the observation comes from?

2. **Try to implement at least 2 different classification methods to answer your research question.**

1. Logistic regression
2. Decision tree

3. **Reflect on the information gained from these two methods and how you might justify this method to others.**

#### Your Work {-}

```{r}
# library statements 
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(tidymodels)
library(lubridate)
library(gridExtra)
library(probably)
tidymodels_prefer() # Resolves conflicts, prefers tidymodel functions

# read in data
covid <- read_csv("owid-covid-data.csv")
```

```{r}
# data cleaning
covid <- covid %>%
  select(total_cases, new_cases, date, total_deaths, reproduction_rate, people_vaccinated_per_hundred, people_fully_vaccinated_per_hundred, total_boosters_per_hundred, tests_per_case, icu_patients, positive_rate, life_expectancy, population_density, median_age, gdp_per_capita, continent) %>%
  filter(date >= as.Date("2021-08-13"))%>%
  na.omit(covid)
```

- We're considering COVID cases occurring after the first recorded booster vaccines (13th August, 2021) to best capture today's landscape.  
- This is when vaccinations started to become widespread (at least in the United States).
- In addition to our existing variables, we also added the following to our model to predict the continent:
    - `life_expectancy`
    - `population_density`
    - `median_age`
    - `gdp_per_capita`

## Classification - Methods

### LASSO Logistic Regression 

We wil fit a LASSO logistic regression model for the `continent` outcome, and allow all possible predictors to be considered.

We initially try a sequence of 100 $\lambda$'s from 1 to 10, which we then adjust based on the plot of test AUC versus $\lambda$.

Since we can only perform binominal logistic regression, we had to create a new variable, `NorthAmerica_or_Not`, categorizing the data based on whether its `continent` was "North America" or not. This new variable will be the outcome we try to predict using this method, setting the reference level as not North America.

To fit the model, we had to remove `date` and `continent` as predictors, since their formats did not work with the engine.

```{r}
# Create new binary variable
covid <-covid %>% 
  mutate(NorthAmerica_or_Not = ifelse(continent == 'North America', 'North_America', 'Not_North_America'))

# Set reference level (to the outcome you are NOT interested in)
covid <- covid %>%
  mutate(NorthAmerica_or_Not = relevel(factor(NorthAmerica_or_Not), ref='Not_North_America')) #set reference level

# creation of cv folds
set.seed(123)
covid_cv10 <- vfold_cv(covid, v = 10)

# Logistic LASSO Regression Model Spec
logistic_lasso_spec_tune <- logistic_reg() %>%
    set_engine('glmnet') %>%
    set_args(mixture = 1, penalty = tune()) %>%
    set_mode('classification')

# Recipe
logistic_rec <- recipe(NorthAmerica_or_Not ~ total_cases + new_cases + total_deaths + reproduction_rate + people_vaccinated_per_hundred + people_fully_vaccinated_per_hundred + total_boosters_per_hundred + tests_per_case + icu_patients + positive_rate + life_expectancy + population_density + median_age + gdp_per_capita, data = covid) %>%
    step_normalize(all_numeric_predictors()) %>% 
    step_dummy(all_nominal_predictors())

# Workflow (Recipe + Model)
log_lasso_wf <- workflow() %>% 
    add_recipe(logistic_rec) %>%
    add_model(logistic_lasso_spec_tune) 

# Tune Model (trying a variety of values of Lambda penalty)
penalty_grid <- grid_regular(
  penalty(range = c(-5, 1)), #log10 transformed  (kept moving min down from 0)
  levels = 100)

tune_output <- tune_grid( 
  log_lasso_wf, # workflow
  resamples = covid_cv10, # cv folds
  metrics = metric_set(roc_auc,accuracy),
  control = control_resamples(save_pred = TRUE, event_level = 'second'),
  grid = penalty_grid # penalty grid defined above
)

# Visualize Model Evaluation Metrics from Tuning
autoplot(tune_output) + theme_classic()
```


#### Inspecting the model 

We will choose the final model whose CV AUC is within one standard error of the overall best metric. 

```{r}
# Select Penalty
best_se_penalty <- select_by_one_std_err(tune_output, metric = 'roc_auc', desc(penalty)) # choose penalty value based on the largest penalty within 1 se of the highest CV roc_auc
best_se_penalty

# Fit Final Model
final_fit_se <- finalize_workflow(log_lasso_wf, best_se_penalty) %>% # incorporates penalty value to workflow 
    fit(data = covid)

final_fit_se %>% tidy() %>%
  filter(estimate == 0)
```
With the set of penalty values that we used, the following variables had their coefficients set to 0:
- `total_cases`
- `people_fully_vaccinated_per_hundred`

Apparently, the total number of cases and number of people fully vaccinated per hundred were not that important in predicting whether or not an observation originated from North America or not, after accounting for the other features in the model.

Here we will look at how long a variable stayed in the model as a measure of variable importance:

```{r}
glmnet_output <- final_fit_se %>% extract_fit_engine()
    
# Create a boolean matrix (predictors x lambdas) of variable exclusion
bool_predictor_exclude <- glmnet_output$beta==0

# Loop over each variable
var_imp <- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) {
    # Extract coefficient path (sorted from highest to lowest lambda)
    this_coeff_path <- bool_predictor_exclude[row,]
    # Compute and return the # of lambdas until this variable is out forever
    ncol(bool_predictor_exclude) - which.min(this_coeff_path) + 1
})

# Create a dataset of this information and sort
var_imp_data <- tibble(
    var_name = rownames(bool_predictor_exclude),
    var_imp = var_imp
)
var_imp_data %>% arrange(desc(var_imp))
```

Based on the output above, the variables deemed the most important in predicting the continent based on how long they persisted in the LASSO algorithm as the penalty increased. 

Surprisingly, `total_cases` and `people_fully_vaccinated_per_hundred` *are* important!

#### Interpreting evaluation metrics

We inspect the overall CV results for the "best" $\lambda$, and compute the no-information (NIR). 

```{r}
# CV results for "best lambda"
tune_output %>%
    collect_metrics() %>%
    filter(penalty == best_se_penalty %>% pull(penalty))
           
# Count up the continent outcomes
covid %>% count(NorthAmerica_or_Not) # Name of the outcome variable goes inside count()

# Compute the NIR
2596/(2596+325)
```

The relative frequency of the majority class is 0.89, meaning 89% of our observations originate from outside of North America. 

The AUC for our model is 1.0! Apparently, our model is a perfect classifier of whether or not a case originated from North America!

#### Choosing a threshold and final model

After using LASSO and balancing bias and variance, we use the final model to make predictions: 

```{r}
# Soft Predictions on Training Data
final_output <- final_fit_se %>% predict(new_data = covid, type='prob') %>% bind_cols(covid)

# FIX THIS
final_output %>%
  ggplot(aes(x = NorthAmerica_or_Not, y = .pred_North_America)) +
  geom_boxplot()

# ROC curve of sensitivity and specificity
final_output %>%
    roc_curve(NorthAmerica_or_Not,.pred_North_America,event_level = 'second') %>%
    autoplot()
```

This ROC curve is 1.0! Apparently, our model is a perfect classifier with sensitivity of 1. 

Sensitivity is the measure of how North-American continents are correctly predicted as North-American by our model. 

Specificity is measure of how often non-North-American continents are correctly predicted to be not North-America by our model. 

We can also calculate the J index:

```{r}
# thresholds in terms of reference level
threshold_output <- final_output %>%
    threshold_perf(truth = NorthAmerica_or_Not, estimate = .pred_North_America, thresholds = seq(0,1,by=.01)) 

# J-index v. threshold for not_spam
threshold_output %>%
    filter(.metric == 'j_index') %>%
    ggplot(aes(x = .threshold, y = .estimate)) +
    geom_line() +
    labs(y = 'J-index', x = 'threshold') +
    theme_classic()
```

```{r}
threshold_output %>%
    filter(.metric == 'j_index') %>%
    arrange(desc(.estimate))
```

```{r}
# Distance v. threshold for not_North_America

threshold_output %>%
    filter(.metric == 'distance') %>%
    ggplot(aes(x = .threshold, y = .estimate)) +
    geom_line() +
    labs(y = 'Distance', x = 'threshold') +
    theme_classic()
```

```{r}
threshold_output %>%
    filter(.metric == 'distance') %>%
    arrange(.estimate)
```

```{r}
log_metrics <- metric_set(accuracy,sens,yardstick::spec)

final_output %>%
    mutate(.pred_class = make_two_class_pred(.pred_Not_North_America, levels(NorthAmerica_or_Not), threshold = .64)) %>%
    log_metrics(truth = NorthAmerica_or_Not, estimate = .pred_class, event_level = 'second')
```

99% of COVID case classifications into North America or not are expected to be correct under this model - wow!

Our NIR is 88%, so our model performs better and is actually almost 100% accurate...! However, most of our cases do not actually originate from North America...

## K-means Clustering

In the code below, we construct a dendrogram using the quantitative variables in our dataset to predict whether or not a case originates from North America. This approach is a visual way to explore clustering and how clusters form from grouping on the quantitative characteristics of the countries where cases were recorded, e.g. the tests per case. If the clustering didn't 

```{r}
# Random subsample of 50 cases
set.seed(123)

covid <- covid %>%
  slice_sample(n = 100)

# ASK BRYAN: SHOULD WE FIX THE DATE 
# select the variables to be used in clustering

covid_sub <- covid %>% 
  select(total_cases, people_fully_vaccinated_per_hundred)
```

### Picking k

The **Elbow Method** is based on the visualization below. We pick the $k$ that corresponds to the bend in the curve (the "elbow") such that you are minimizing total within-cluster sum of squares while keeping the number of clusters small (simpler, more interpretable).

This enables us to pick $k$ using a data-driven approach. We would be comparing the **total squared distance of each case from its assigned centroid** for different values of $k$. (This measure is available within the `$tot.withinss` component of objects resulting from `kmeans()`.)

Based on the visualization below, we pick $k = 6$. After this, there is not as much meaningful decrease in heterogeneity (dissimilarity).

This makes a lot of sense because there are 6 continents where cases are observed in the data set.
```{r}
# Data-specific function to cluster and calculate total within-cluster SS
covid_cluster_ss <- function(k){
    # Perform clustering
    kclust <- kmeans(scale(covid_sub), centers = k)

    # Return the total within-cluster sum of squares
    return(kclust$tot.withinss)
}

tibble(
    k = 1:15,
    tot_wc_ss = purrr::map_dbl(1:15, covid_cluster_ss)
) %>% 
    ggplot(aes(x = k, y = tot_wc_ss)) +
    geom_point() + 
    labs(x = "Number of clusters",y = 'Total within-cluster sum of squares') + 
    theme_classic()
```

### Cluster interpretation

```{r}
# Perform clustering: should you use scale()?
set.seed(123)
kclust_k6_2vars <- kmeans(scale(covid_sub), centers = 6)

covid <- covid %>%
    mutate(kclust_6_2vars = factor(kclust_k6_2vars$cluster))


covid %>%
  count(continent,kclust_6_2vars)

covid %>%
    group_by(kclust_6_2vars) %>%
    summarize(across(c(total_cases, people_fully_vaccinated_per_hundred), mean))
```

Group 3 has the highest mean total cases and the second lowest mean total vaccination rate. Group 5 could be named low case rate and low vaccination rate as it has the lowest number of total cases and the lowest rate of people fully vaccinated. Group 6 has the highest number of people fully vaccinated per hundred but also has the third lowest total cases which is surprising to us but this is indicative that other factors can affect clustering. 

### Clustering Visualization

```{r}
# Run k-means for k = centers = 6
kclust_k6 <- kmeans(scale(covid_sub), centers = 6)

# Add a variable (kclust_k3) to the original dataset 
# containing the cluster assignments
covid_sub <- covid_sub %>%
    mutate(kclust_6 = factor(kclust_k6$cluster))

# Visualize the cluster assignments on the original scatterplot
ggplot(covid_sub, aes(x = total_cases, y = people_fully_vaccinated_per_hundred, color = kclust_6)) +
    geom_point() + 
    theme_classic()
```

### Clustering Evalution

- The two features we chose in the clustering were `total_cases` and `people_fully_vaccinated_per_hundred`. We chose these variables because the variable importance calculation from the LASSO logistic regression done previously picked these two variables as the most important. Aditionally, the clustering can only be done for quantitative variables since we are able to measure the distance between them.
- It looks like there are 6 clusters with varying vaccination rate per hundred people and total number of cases.

## Homework Prompts

1. **Indicate at least 2 different methods used to answer your classification research question.**
2. **Describe what you did to evaluate the models explored.**
3. **Indicate how you estimated quantitative evaluation metrics.**
4. **Describe the goals / purpose of the methods used in the overall context of your research investigations.**

## Classification - Results

1. **Summarize your final model and justify your model choice (see below for ways to justify your choice)**.
2. **Compare the different classification models tried in light of evaluation metrics, variable importance, and data context.**
3. **Display evaluation metrics for different models in a clean, organized way. This display should include both the estimated metric as well as its standard deviation. (This won’t be available from OOB error estimation. If using OOB, don’t worry about reporting the SD.)**
4. **Broadly summarize conclusions from looking at these evaluation metrics and their measures of uncertainty.**

## Classification - Conclusions

1. **Interpret evaluation metric(s) for the final model in context. Does the model show an acceptable amount of error?**
2. **If using OOB error estimation, display the test (OOB) confusion matrix, and use it to interpret the strengths and weaknesses of the final model.**
3. **Summarization should show evidence of acknowledging the data context in thinking about the sensibility of these results.**
